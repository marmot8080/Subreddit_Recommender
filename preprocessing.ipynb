{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "437fbc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.6.1 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (1.6.1)\n",
      "Requirement already satisfied: pandas==2.2.3 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: numpy==2.1.3 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (2.1.3)\n",
      "Requirement already satisfied: matplotlib==3.10.0 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (3.10.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from scikit-learn==1.6.1->-r requirements.txt (line 1)) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from scikit-learn==1.6.1->-r requirements.txt (line 1)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from scikit-learn==1.6.1->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from pandas==2.2.3->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from pandas==2.2.3->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from pandas==2.2.3->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from matplotlib==3.10.0->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from matplotlib==3.10.0->-r requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from matplotlib==3.10.0->-r requirements.txt (line 4)) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from matplotlib==3.10.0->-r requirements.txt (line 4)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from matplotlib==3.10.0->-r requirements.txt (line 4)) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from matplotlib==3.10.0->-r requirements.txt (line 4)) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from matplotlib==3.10.0->-r requirements.txt (line 4)) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.2.3->-r requirements.txt (line 2)) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95f80227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebfa0e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'assets/' \n",
    "MODEL_PATH = 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9c7e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리에 nltk의 stopwords 활용\n",
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "    english_stopwords = stopwords.words('english')\n",
    "except ImportError:\n",
    "    print(\"경고: nltk 라이브러리가 설치되지 않았거나 'stopwords'가 다운로드되지 않았습니다.\")\n",
    "    english_stopwords = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cbde94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서브레딧 및 그룹 정의\n",
    "SUB_PER_GROUP = 4\n",
    "SUBREDDITS = [\n",
    "    'Thetruthishere', 'Glitch_in_the_Matrix', 'UnresolvedMysteries', 'Paranormal',\n",
    "    'learnprogramming', 'cscareerquestions', 'SideProject', 'AskProgramming',\n",
    "    'TrueFilm', 'booksuggestions', 'TrueGaming', 'LetsTalkMusic',\n",
    "    'relationship_advice', 'AmItheAsshole', 'offmychest', 'Advice',\n",
    "    'personalfinance', 'investing', 'Frugal', 'financialindependence',\n",
    "]\n",
    "GROUP_MAP = {\n",
    "    'Mystery': SUBREDDITS[0:SUB_PER_GROUP], \n",
    "    'Dev': SUBREDDITS[SUB_PER_GROUP:2*SUB_PER_GROUP], \n",
    "    'Culture': SUBREDDITS[2*SUB_PER_GROUP:3*SUB_PER_GROUP], \n",
    "    'Life': SUBREDDITS[3*SUB_PER_GROUP:4*SUB_PER_GROUP],\n",
    "    'Finance': SUBREDDITS[4*SUB_PER_GROUP:5*SUB_PER_GROUP],\n",
    "}\n",
    "VECTOR_DIMENSION = 5000  # 문서 벡터 차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcd6f619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thetruthishere</td>\n",
       "      <td>about truth Leviathan</td>\n",
       "      <td>about Leviathan isn't scary monster is actuall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thetruthishere</td>\n",
       "      <td>Weird nightmare last night</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thetruthishere</td>\n",
       "      <td>The streetlight in front of my house turns off...</td>\n",
       "      <td>For the past month, the streetlight directly o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thetruthishere</td>\n",
       "      <td>The reflection in the window was a few seconds...</td>\n",
       "      <td>I was on a nearly empty late-night train, star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thetruthishere</td>\n",
       "      <td>Strange knocking in my empty apartment at night.</td>\n",
       "      <td>I've been living alone in this old apartment f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        subreddit                                              title  \\\n",
       "0  Thetruthishere                              about truth Leviathan   \n",
       "1  Thetruthishere                         Weird nightmare last night   \n",
       "2  Thetruthishere  The streetlight in front of my house turns off...   \n",
       "3  Thetruthishere  The reflection in the window was a few seconds...   \n",
       "4  Thetruthishere   Strange knocking in my empty apartment at night.   \n",
       "\n",
       "                                                text  \n",
       "0  about Leviathan isn't scary monster is actuall...  \n",
       "1                                                NaN  \n",
       "2  For the past month, the streetlight directly o...  \n",
       "3  I was on a nearly empty late-night train, star...  \n",
       "4  I've been living alone in this old apartment f...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv(FILE_PATH + 'reddit_posts.csv')\n",
    "\n",
    "subreddit_df = df[df['subreddit'] == SUBREDDITS[0]].copy()\n",
    "subreddit_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ca2c67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- r/Thetruthishere ---\n",
      "전체 데이터 개수: 1000\n",
      "text가 NaN인 데이터 개수: 45\n",
      "\n",
      "--- r/Glitch_in_the_Matrix ---\n",
      "전체 데이터 개수: 1000\n",
      "text가 NaN인 데이터 개수: 9\n",
      "\n",
      "--- r/UnresolvedMysteries ---\n",
      "전체 데이터 개수: 1000\n",
      "text가 NaN인 데이터 개수: 0\n",
      "\n",
      "--- r/Paranormal ---\n",
      "전체 데이터 개수: 1000\n",
      "text가 NaN인 데이터 개수: 3\n",
      "\n",
      "--- r/learnprogramming ---\n",
      "전체 데이터 개수: 1000\n",
      "text가 NaN인 데이터 개수: 0\n",
      "\n",
      "--- r/cscareerquestions ---\n",
      "전체 데이터 개수: 1000\n",
      "text가 NaN인 데이터 개수: 0\n",
      "\n",
      "--- r/SideProject ---\n",
      "전체 데이터 개수: 1000\n",
      "text가 NaN인 데이터 개수: 53\n",
      "\n",
      "--- r/AskProgramming ---\n",
      "전체 데이터 개수: 988\n",
      "text가 NaN인 데이터 개수: 32\n",
      "\n",
      "--- r/TrueFilm ---\n",
      "전체 데이터 개수: 1000\n",
      "text가 NaN인 데이터 개수: 0\n",
      "\n",
      "--- r/booksuggestions ---\n",
      "전체 데이터 개수: 1000\n",
      "text가 NaN인 데이터 개수: 0\n",
      "\n",
      "--- r/TrueGaming ---\n",
      "전체 데이터 개수: 1000\n",
      "text가 NaN인 데이터 개수: 3\n",
      "\n",
      "--- r/LetsTalkMusic ---\n",
      "전체 데이터 개수: 995\n",
      "text가 NaN인 데이터 개수: 0\n",
      "\n",
      "--- r/relationship_advice ---\n",
      "전체 데이터 개수: 1000\n",
      "text가 NaN인 데이터 개수: 0\n",
      "\n",
      "--- r/AmItheAsshole ---\n",
      "전체 데이터 개수: 979\n",
      "text가 NaN인 데이터 개수: 0\n",
      "\n",
      "--- r/offmychest ---\n",
      "전체 데이터 개수: 1000\n",
      "text가 NaN인 데이터 개수: 0\n",
      "\n",
      "--- r/Advice ---\n",
      "전체 데이터 개수: 1000\n",
      "text가 NaN인 데이터 개수: 24\n",
      "\n",
      "--- r/personalfinance ---\n",
      "전체 데이터 개수: 990\n",
      "text가 NaN인 데이터 개수: 43\n",
      "\n",
      "--- r/investing ---\n",
      "전체 데이터 개수: 962\n",
      "text가 NaN인 데이터 개수: 0\n",
      "\n",
      "--- r/Frugal ---\n",
      "전체 데이터 개수: 991\n",
      "text가 NaN인 데이터 개수: 12\n",
      "\n",
      "--- r/financialindependence ---\n",
      "전체 데이터 개수: 996\n",
      "text가 NaN인 데이터 개수: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for subreddit in SUBREDDITS:\n",
    "    subreddit_df = df[df['subreddit'] == subreddit]\n",
    "    total_count = len(subreddit_df)\n",
    "    nan_count = subreddit_df['text'].isna().sum()\n",
    "    print(f\"--- r/{subreddit} ---\")\n",
    "    print(f\"전체 데이터 개수: {total_count}\")\n",
    "    print(f\"text가 NaN인 데이터 개수: {nan_count}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4153310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 결측값 개수: 226\n",
      "결측값 제거 후 결측값 개수: 0\n"
     ]
    }
   ],
   "source": [
    "# 결측값 제거\n",
    "print(f\"총 결측값 개수: {df['text'].isna().sum()}\")\n",
    "df.dropna(subset=['text'], inplace=True)\n",
    "print(f\"결측값 제거 후 결측값 개수: {df['text'].isna().sum()}\")  # 결측값이 제거되었는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35b358c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[저장 완료]\n",
      "Train 데이터 개수: 15736 -> assets/train_data.csv\n",
      "Test 데이터 개수: 3934 -> assets/test_data.csv\n",
      "\n",
      "[Train Data Head]\n",
      "                 subreddit                               preprocessed_content\n",
      "13527        AmItheAsshole  aita using ai write college assignment hey f s...\n",
      "10562       AskProgramming  dont want share github page others hey ive pro...\n",
      "16659      personalfinance  started receiving annuity checks name parent p...\n",
      "12822  relationship_advice  girlfriend f struggling throughout long distan...\n",
      "11536        LetsTalkMusic  importance tom moulton tom moulton responsible...\n"
     ]
    }
   ],
   "source": [
    "# 효율적인 불용어 처리를 위해 리스트를 집합(set)으로 변환\n",
    "stopwords_set = set(english_stopwords)\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    텍스트 데이터를 전처리하는 함수:\n",
    "    1. 소문자 변환\n",
    "    2. 알파벳과 공백을 제외한 모든 문자 제거\n",
    "    3. 불용어 제거\n",
    "    \"\"\"\n",
    "    # 입력값이 문자열이 아닌 경우 빈 문자열 반환\n",
    "    if not isinstance(text, str):\n",
    "        return np.nan\n",
    "    \n",
    "    # url 제거\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # 정규표현식을 사용하여 알파벳과 공백 외의 문자 제거 및 소문자 변환\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "    \n",
    "    # 공백을 기준으로 단어 토큰화 후 불용어 제거\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stopwords_set]\n",
    "\n",
    "    # 결과가 없으면(모두 불용어거나 특수문자였으면) NaN 반환\n",
    "    if not filtered_words:\n",
    "        return np.nan\n",
    "    \n",
    "    # 처리된 단어들을 다시 하나의 문자열로 결합\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "preprocessed_df = pd.read_csv(FILE_PATH + 'reddit_posts.csv')\n",
    "\n",
    "# 'title'과 'text' 열을 결합하여 'content' 열 생성\n",
    "preprocessed_df['content'] = preprocessed_df['title'] + \" \" + preprocessed_df['text']\n",
    "\n",
    "# 'content' 열에 전처리 함수 적용하여 'processed_content' 열 생성\n",
    "preprocessed_df['preprocessed_content'] = preprocessed_df['content'].apply(preprocess_text)\n",
    "\n",
    "# 원본 'content', 'title', 'text' 열 제거\n",
    "preprocessed_df.drop(columns=['content', 'title', 'text'], inplace=True)\n",
    "\n",
    "# 전처리 후 결측값 제거\n",
    "preprocessed_df.dropna(subset=['preprocessed_content'], inplace=True)\n",
    "\n",
    "# 데이터 분할 (8:2 비율)\n",
    "train_df, test_df = train_test_split(\n",
    "    preprocessed_df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=preprocessed_df['subreddit']\n",
    ")\n",
    "\n",
    "# 분할된 데이터 저장\n",
    "train_df.to_csv(FILE_PATH + 'train_data.csv', index=False)\n",
    "test_df.to_csv(FILE_PATH + 'test_data.csv', index=False)\n",
    "\n",
    "# 전처리 결과 확인\n",
    "print(f\"\\n[저장 완료]\")\n",
    "print(f\"Train 데이터 개수: {len(train_df)} -> {FILE_PATH}train_data.csv\")\n",
    "print(f\"Test 데이터 개수: {len(test_df)} -> {FILE_PATH}test_data.csv\")\n",
    "\n",
    "# Train 데이터 상위 5개 확인\n",
    "print(\"\\n[Train Data Head]\")\n",
    "print(train_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83ef1269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "# 벡터라이저 저장\n",
    "vectorizer = TfidfVectorizer(max_features=VECTOR_DIMENSION, ngram_range=(1, 2))\n",
    "vectorizer.fit_transform(train_df['preprocessed_content']).toarray()\n",
    "\n",
    "joblib.dump(vectorizer, MODEL_PATH + 'tfidf_vectorizer.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
