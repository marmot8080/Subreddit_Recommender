{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f80227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa0e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'assets/' \n",
    "MODEL_PATH = 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c7e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리에 nltk의 stopwords 활용\n",
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "    english_stopwords = stopwords.words('english')\n",
    "except ImportError:\n",
    "    print(\"경고: nltk 라이브러리가 설치되지 않았거나 'stopwords'가 다운로드되지 않았습니다.\")\n",
    "    english_stopwords = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cbde94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서브레딧 및 그룹 정의\n",
    "SUB_PER_GROUP = 4\n",
    "SUBREDDITS = [\n",
    "    'Thetruthishere', 'Glitch_in_the_Matrix', 'UnresolvedMysteries', 'Paranormal',\n",
    "    'learnprogramming', 'cscareerquestions', 'SideProject', 'AskProgramming',\n",
    "    'TrueFilm', 'booksuggestions', 'TrueGaming', 'LetsTalkMusic',\n",
    "    'relationship_advice', 'AmItheAsshole', 'offmychest', 'Advice',\n",
    "    'personalfinance', 'investing', 'Frugal', 'financialindependence',\n",
    "]\n",
    "GROUP_MAP = {\n",
    "    'Mystery': SUBREDDITS[0:SUB_PER_GROUP], \n",
    "    'Dev': SUBREDDITS[SUB_PER_GROUP:2*SUB_PER_GROUP], \n",
    "    'Culture': SUBREDDITS[2*SUB_PER_GROUP:3*SUB_PER_GROUP], \n",
    "    'Life': SUBREDDITS[3*SUB_PER_GROUP:4*SUB_PER_GROUP],\n",
    "    'Finance': SUBREDDITS[4*SUB_PER_GROUP:5*SUB_PER_GROUP],\n",
    "}\n",
    "VECTOR_DIMENSION = 5000  # 문서 벡터 차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd6f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv(FILE_PATH + 'reddit_posts.csv')\n",
    "\n",
    "subreddit_df = df[df['subreddit'] == SUBREDDITS[0]].copy()\n",
    "subreddit_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca2c67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subreddit in SUBREDDITS:\n",
    "    subreddit_df = df[df['subreddit'] == subreddit]\n",
    "    total_count = len(subreddit_df)\n",
    "    nan_count = subreddit_df['text'].isna().sum()\n",
    "    print(f\"--- r/{subreddit} ---\")\n",
    "    print(f\"전체 데이터 개수: {total_count}\")\n",
    "    print(f\"text가 NaN인 데이터 개수: {nan_count}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4153310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값 제거\n",
    "print(f\"총 결측값 개수: {df['text'].isna().sum()}\")\n",
    "df.dropna(subset=['text'], inplace=True)\n",
    "print(f\"결측값 제거 후 결측값 개수: {df['text'].isna().sum()}\")  # 결측값이 제거되었는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b358c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 효율적인 불용어 처리를 위해 리스트를 집합(set)으로 변환\n",
    "stopwords_set = set(english_stopwords)\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    텍스트 데이터를 전처리하는 함수:\n",
    "    1. 소문자 변환\n",
    "    2. 알파벳과 공백을 제외한 모든 문자 제거\n",
    "    3. 불용어 제거\n",
    "    \"\"\"\n",
    "    # 입력값이 문자열이 아닌 경우 빈 문자열 반환\n",
    "    if not isinstance(text, str):\n",
    "        return np.nan\n",
    "    \n",
    "    # url 제거\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # 정규표현식을 사용하여 알파벳과 공백 외의 문자 제거 및 소문자 변환\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "    \n",
    "    # 공백을 기준으로 단어 토큰화 후 불용어 제거\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stopwords_set]\n",
    "\n",
    "    # 결과가 없으면(모두 불용어거나 특수문자였으면) NaN 반환\n",
    "    if not filtered_words:\n",
    "        return np.nan\n",
    "    \n",
    "    # 처리된 단어들을 다시 하나의 문자열로 결합\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "preprocessed_df = pd.read_csv(FILE_PATH + 'reddit_posts.csv')\n",
    "\n",
    "# 'title'과 'text' 열을 결합하여 'content' 열 생성\n",
    "preprocessed_df['content'] = preprocessed_df['title'] + \" \" + preprocessed_df['text']\n",
    "\n",
    "# 'content' 열에 전처리 함수 적용하여 'processed_content' 열 생성\n",
    "preprocessed_df['preprocessed_content'] = preprocessed_df['content'].apply(preprocess_text)\n",
    "\n",
    "# 원본 'content', 'title', 'text' 열 제거\n",
    "preprocessed_df.drop(columns=['content', 'title', 'text'], inplace=True)\n",
    "\n",
    "# 전처리 후 결측값 제거\n",
    "preprocessed_df.dropna(subset=['preprocessed_content'], inplace=True)\n",
    "\n",
    "# 데이터 분할 (8:2 비율)\n",
    "train_df, test_df = train_test_split(\n",
    "    preprocessed_df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=preprocessed_df['subreddit']\n",
    ")\n",
    "\n",
    "# 분할된 데이터 저장\n",
    "train_df.to_csv(FILE_PATH + 'train_data.csv', index=False)\n",
    "test_df.to_csv(FILE_PATH + 'test_data.csv', index=False)\n",
    "\n",
    "# 전처리 결과 확인\n",
    "print(f\"\\n[저장 완료]\")\n",
    "print(f\"Train 데이터 개수: {len(train_df)} -> {FILE_PATH}train_data.csv\")\n",
    "print(f\"Test 데이터 개수: {len(test_df)} -> {FILE_PATH}test_data.csv\")\n",
    "\n",
    "# Train 데이터 상위 5개 확인\n",
    "print(\"\\n[Train Data Head]\")\n",
    "print(train_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ef1269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "# 벡터라이저 저장\n",
    "vectorizer = TfidfVectorizer(max_features=VECTOR_DIMENSION, ngram_range=(1, 2))\n",
    "vectorizer.fit_transform(train_df['preprocessed_content']).toarray()\n",
    "\n",
    "joblib.dump(vectorizer, MODEL_PATH + 'tfidf_vectorizer.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
