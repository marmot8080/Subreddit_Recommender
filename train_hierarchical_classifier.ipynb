{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766a0d7c",
   "metadata": {},
   "source": [
    "# 계층형 분류기 학습\n",
    "\n",
    "본 프로젝트에서는 텍스트 데이터를 2단계에 걸쳐 분류하는 계층형 분류기(Hierarchical Classifier)를 구현합니다. 이 접근 방식은 대분류(그룹)를 먼저 판별한 후, 해당 그룹 내에서 세부 분류(서브레딧)를 수행하여 전체적인 분류 정확도와 효율성을 높이는 것을 목표로 합니다.\n",
    "\n",
    "## 학습 과정\n",
    "\n",
    "1.  **데이터 준비**: \n",
    "    *   **데이터 확인**: 데이터 형태와 결측값 개수 등을 확인합니다.\n",
    "    *   **전처리**: 결측값 및 불용어 제거 등의 전처리를 수행합니다.\n",
    "    *   **벡터화**: 전처리된 텍스트 데이터를 `TfidfVectorizer`를 사용하여 고차원 벡터로 변환합니다. 이를 통해 텍스트의 의미적 특징을 수치적으로 표현합니다.\n",
    "\n",
    "2.  **상위 분류기 (Top-level Classifier) 학습**:\n",
    "    *   **목표**: 텍스트가 3개의 그룹('Mystery', 'Dev', 'Culture') 중 어디에 속하는지 분류합니다.\n",
    "    *   **모델**: 대용량 텍스트 데이터에 효율적인 `SGDClassifier`를 사용합니다.\n",
    "    *   **최적화**: `RandomizedSearchCV`를 이용해 최적의 하이퍼파라미터를 탐색하여 모델의 일반화 성능을 극대화합니다.\n",
    "\n",
    "3.  **하위 분류기 (Sub-level Classifiers) 학습**:\n",
    "    *   **목표**: 각 그룹별로 3개의 서브레딧 중 어느 것인지 세부적으로 분류합니다.\n",
    "    *   **모델**: 그룹별로 독립된 `SGDClassifier`를 학습시켜, 해당 그룹의 데이터 특성에 더 전문화된 모델을 만듭니다.\n",
    "    *   **최적화**: 각 하위 분류기 역시 `RandomizedSearchCV`를 통해 개별적으로 최적화합니다.\n",
    "\n",
    "4.  **모델 저장**: 학습이 완료된 상위 분류기와 3개의 하위 분류기 모델을 각각 파일로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00f75eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.6.1 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (1.6.1)\n",
      "Requirement already satisfied: pandas==2.2.3 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: numpy==2.1.3 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (2.1.3)\n",
      "Requirement already satisfied: matplotlib==3.10.0 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (3.10.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from scikit-learn==1.6.1->-r requirements.txt (line 1)) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from scikit-learn==1.6.1->-r requirements.txt (line 1)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from scikit-learn==1.6.1->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from pandas==2.2.3->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from pandas==2.2.3->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from pandas==2.2.3->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from matplotlib==3.10.0->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from matplotlib==3.10.0->-r requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from matplotlib==3.10.0->-r requirements.txt (line 4)) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from matplotlib==3.10.0->-r requirements.txt (line 4)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from matplotlib==3.10.0->-r requirements.txt (line 4)) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from matplotlib==3.10.0->-r requirements.txt (line 4)) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from matplotlib==3.10.0->-r requirements.txt (line 4)) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\overl\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.2.3->-r requirements.txt (line 2)) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e137ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "173a5032",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'assets/' \n",
    "MODEL_PATH = 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70ca9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리에 nltk의 stopwords 활용\n",
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "    english_stopwords = stopwords.words('english')\n",
    "except ImportError:\n",
    "    print(\"경고: nltk 라이브러리가 설치되지 않았거나 'stopwords'가 다운로드되지 않았습니다.\")\n",
    "    english_stopwords = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993dfda9",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비\n",
    "\n",
    "### 서브레딧 목록 (총 9개, 3그룹)\n",
    "\n",
    "수집 데이터의 다양성을 확보하고 유사 범주별 관계성을 분석하기 위해 아래와 같이 3개 그룹, 총 9개의 서브레딧을 선정했습니다.\n",
    "\n",
    "그룹 | 주제 | 대상 서브레딧\n",
    "-- | -- | --\n",
    "A | 미스터리 및 특이 현상 | r/Thetruthishere, r/Glitch_in_the_Matrix, r/UnresolvedMysteries\n",
    "B | 개발 및 커리어 | r/learnprogramming, r/cscareerquestions, r/SideProject\n",
    "C | 문화 및 심층 분석 | r/TrueFilm, r/booksuggestions, r/TrueGaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17cf6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서브레딧 및 그룹 정의\n",
    "SUBREDDITS = [\n",
    "    'Thetruthishere', 'Glitch_in_the_Matrix', 'UnresolvedMysteries',\n",
    "    'learnprogramming', 'cscareerquestions', 'SideProject',\n",
    "    'TrueFilm', 'booksuggestions', 'TrueGaming'\n",
    "]\n",
    "GROUP_MAP = {'Mystery': SUBREDDITS[0:3], 'Dev': SUBREDDITS[3:6], 'Culture': SUBREDDITS[6:9]}\n",
    "VECTOR_DIMENSION = 5000  # 문서 벡터 차원\n",
    "N_SELECT = 30   # 센트로이드 정제 시 선별할 상위 N개 문서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a1b2cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thetruthishere</td>\n",
       "      <td>about truth Leviathan</td>\n",
       "      <td>about Leviathan isn't scary monster is actuall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thetruthishere</td>\n",
       "      <td>Weird nightmare last night</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thetruthishere</td>\n",
       "      <td>The streetlight in front of my house turns off...</td>\n",
       "      <td>For the past month, the streetlight directly o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thetruthishere</td>\n",
       "      <td>The reflection in the window was a few seconds...</td>\n",
       "      <td>I was on a nearly empty late-night train, star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thetruthishere</td>\n",
       "      <td>Strange knocking in my empty apartment at night.</td>\n",
       "      <td>I've been living alone in this old apartment f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        subreddit                                              title  \\\n",
       "0  Thetruthishere                              about truth Leviathan   \n",
       "1  Thetruthishere                         Weird nightmare last night   \n",
       "2  Thetruthishere  The streetlight in front of my house turns off...   \n",
       "3  Thetruthishere  The reflection in the window was a few seconds...   \n",
       "4  Thetruthishere   Strange knocking in my empty apartment at night.   \n",
       "\n",
       "                                                text  \n",
       "0  about Leviathan isn't scary monster is actuall...  \n",
       "1                                                NaN  \n",
       "2  For the past month, the streetlight directly o...  \n",
       "3  I was on a nearly empty late-night train, star...  \n",
       "4  I've been living alone in this old apartment f...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv(FILE_PATH + 'reddit_posts.csv')\n",
    "\n",
    "subreddit_df = df[df['subreddit'] == SUBREDDITS[0]].copy()\n",
    "subreddit_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2eb936e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- r/Thetruthishere ---\n",
      "전체 데이터 개수: 995\n",
      "text가 NaN인 데이터 개수: 45\n",
      "\n",
      "--- r/Glitch_in_the_Matrix ---\n",
      "전체 데이터 개수: 996\n",
      "text가 NaN인 데이터 개수: 9\n",
      "\n",
      "--- r/UnresolvedMysteries ---\n",
      "전체 데이터 개수: 991\n",
      "text가 NaN인 데이터 개수: 0\n",
      "\n",
      "--- r/learnprogramming ---\n",
      "전체 데이터 개수: 995\n",
      "text가 NaN인 데이터 개수: 0\n",
      "\n",
      "--- r/cscareerquestions ---\n",
      "전체 데이터 개수: 987\n",
      "text가 NaN인 데이터 개수: 0\n",
      "\n",
      "--- r/SideProject ---\n",
      "전체 데이터 개수: 999\n",
      "text가 NaN인 데이터 개수: 53\n",
      "\n",
      "--- r/TrueFilm ---\n",
      "전체 데이터 개수: 988\n",
      "text가 NaN인 데이터 개수: 0\n",
      "\n",
      "--- r/booksuggestions ---\n",
      "전체 데이터 개수: 1000\n",
      "text가 NaN인 데이터 개수: 0\n",
      "\n",
      "--- r/TrueGaming ---\n",
      "전체 데이터 개수: 992\n",
      "text가 NaN인 데이터 개수: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for subreddit in SUBREDDITS:\n",
    "    subreddit_df = df[df['subreddit'] == subreddit]\n",
    "    total_count = len(subreddit_df)\n",
    "    nan_count = subreddit_df['text'].isna().sum()\n",
    "    print(f\"--- r/{subreddit} ---\")\n",
    "    print(f\"전체 데이터 개수: {total_count}\")\n",
    "    print(f\"text가 NaN인 데이터 개수: {nan_count}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec673c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 결측값 개수: 110\n",
      "결측값 제거 후 결측값 개수: 0\n"
     ]
    }
   ],
   "source": [
    "# 결측값 제거\n",
    "print(f\"총 결측값 개수: {df['text'].isna().sum()}\")\n",
    "df.dropna(subset=['text'], inplace=True)\n",
    "print(f\"결측값 제거 후 결측값 개수: {df['text'].isna().sum()}\")  # 결측값이 제거되었는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9aa3baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결측값 개수:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>preprocessed_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thetruthishere</td>\n",
       "      <td>truth leviathan leviathan isnt scary monster a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thetruthishere</td>\n",
       "      <td>streetlight front house turns every time walk ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thetruthishere</td>\n",
       "      <td>reflection window seconds behind nearly empty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thetruthishere</td>\n",
       "      <td>strange knocking empty apartment night ive liv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thetruthishere</td>\n",
       "      <td>whats unexplainable thing youve ever witnessed...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        subreddit                               preprocessed_content\n",
       "0  Thetruthishere  truth leviathan leviathan isnt scary monster a...\n",
       "2  Thetruthishere  streetlight front house turns every time walk ...\n",
       "3  Thetruthishere  reflection window seconds behind nearly empty ...\n",
       "4  Thetruthishere  strange knocking empty apartment night ive liv...\n",
       "5  Thetruthishere  whats unexplainable thing youve ever witnessed..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 효율적인 불용어 처리를 위해 리스트를 집합(set)으로 변환\n",
    "stopwords_set = set(english_stopwords)\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    텍스트 데이터를 전처리하는 함수:\n",
    "    1. 소문자 변환\n",
    "    2. 알파벳과 공백을 제외한 모든 문자 제거\n",
    "    3. 불용어 제거\n",
    "    \"\"\"\n",
    "    # 입력값이 문자열이 아닌 경우 빈 문자열 반환\n",
    "    if not isinstance(text, str):\n",
    "        return np.nan\n",
    "    \n",
    "    # url 제거\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # 정규표현식을 사용하여 알파벳과 공백 외의 문자 제거 및 소문자 변환\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "    \n",
    "    # 공백을 기준으로 단어 토큰화 후 불용어 제거\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stopwords_set]\n",
    "\n",
    "    # 결과가 없으면(모두 불용어거나 특수문자였으면) NaN 반환\n",
    "    if not filtered_words:\n",
    "        return np.nan\n",
    "    \n",
    "    # 처리된 단어들을 다시 하나의 문자열로 결합\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "preprocessed_df = pd.read_csv(FILE_PATH + 'reddit_posts.csv')\n",
    "\n",
    "# 'title'과 'text' 열을 결합하여 'content' 열 생성\n",
    "preprocessed_df['content'] = preprocessed_df['title'] + \" \" + preprocessed_df['text']\n",
    "\n",
    "# 'content' 열에 전처리 함수 적용하여 'processed_content' 열 생성\n",
    "preprocessed_df['preprocessed_content'] = preprocessed_df['content'].apply(preprocess_text)\n",
    "\n",
    "# 원본 'content', 'title', 'text' 열 제거\n",
    "preprocessed_df.drop(columns=['content', 'title', 'text'], inplace=True)\n",
    "\n",
    "# 전처리 후 결측값 제거\n",
    "preprocessed_df.dropna(subset=['preprocessed_content'], inplace=True)\n",
    "\n",
    "# 전처리 결과 저장\n",
    "preprocessed_df.to_csv(FILE_PATH + 'reddit_posts_preprocessed.csv', index=False)\n",
    "\n",
    "# 전처리 결과 확인\n",
    "print('결측값 개수: ', preprocessed_df['preprocessed_content'].isna().sum())\n",
    "preprocessed_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566aa08d",
   "metadata": {},
   "source": [
    "## 2. 상위 분류기 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95de28ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 분할 완료: 학습용 7065개, 테스트용 1767개\n",
      "상위 분류기(Top-level Classifier) 최적화 시작 (RandomizedSearch)...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "\n",
      "==================================================\n",
      "최적의 하이퍼파라미터: {'alpha': np.float64(0.00016480446427978953), 'loss': 'log_loss', 'max_iter': 2000, 'penalty': 'l2'}\n",
      "교차 검증 최고 점수 (Best CV Score): 0.9837\n",
      "--------------------------------------------------\n",
      "최종 테스트 정확도 (Test Accuracy): 0.9870\n",
      "==================================================\n",
      "\n",
      "[상세 분류 리포트]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Culture       0.98      0.98      0.98       595\n",
      "         Dev       0.99      0.99      0.99       586\n",
      "     Mystery       0.99      0.99      0.99       586\n",
      "\n",
      "    accuracy                           0.99      1767\n",
      "   macro avg       0.99      0.99      0.99      1767\n",
      "weighted avg       0.99      0.99      0.99      1767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 상위 그룹 분류기 학습\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "# 벡터화\n",
    "vectorizer = TfidfVectorizer(max_features=VECTOR_DIMENSION, ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(preprocessed_df['preprocessed_content']).toarray()\n",
    "\n",
    "# 상위 그룹 라벨링 (서브레딧 -> 그룹 매핑)\n",
    "reverse_group_map = {subreddit: group for group, subreddits in GROUP_MAP.items() for subreddit in subreddits}\n",
    "y_group = preprocessed_df['subreddit'].map(reverse_group_map)\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_group, test_size=0.2, random_state=42, stratify=y_group\n",
    ")\n",
    "\n",
    "print(f\"데이터 분할 완료: 학습용 {X_train.shape[0]}개, 테스트용 {X_test.shape[0]}개\")\n",
    "\n",
    "\n",
    "# RandomizedSearchCV 설정\n",
    "\n",
    "# 파라미터 분포 정의\n",
    "param_dist = {\n",
    "    'loss': ['log_loss'],  # 로지스틱 회귀 (확률 출력용)\n",
    "    'alpha': loguniform(1e-5, 1e-1), \n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'max_iter': [1000, 2000, 3000]\n",
    "}\n",
    "\n",
    "# 기본 모델 설정\n",
    "base_model = SGDClassifier(random_state=42, class_weight='balanced', n_jobs=1)\n",
    "\n",
    "# 교차 검증 전략 (학습 데이터 안에서만 수행)\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# RandomizedSearchCV 객체 생성\n",
    "random_search_top = RandomizedSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,          \n",
    "    cv=cv_strategy,     \n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1,          \n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"상위 분류기(Top-level Classifier) 최적화 시작 (RandomizedSearch)...\")\n",
    "\n",
    "random_search_top.fit(X_train, y_train)\n",
    "\n",
    "# 결과 출력 및 최종 평가\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"최적의 하이퍼파라미터: {random_search_top.best_params_}\")\n",
    "print(f\"교차 검증 최고 점수 (Best CV Score): {random_search_top.best_score_:.4f}\")\n",
    "\n",
    "# 최적의 모델 추출\n",
    "top_classifier = random_search_top.best_estimator_\n",
    "\n",
    "# 성능 평가\n",
    "y_pred = top_classifier.predict(X_test)\n",
    "final_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"최종 테스트 정확도 (Test Accuracy): {final_accuracy:.4f}\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\n[상세 분류 리포트]\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aad683",
   "metadata": {},
   "source": [
    "## 3. 하위 분류기 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc7acf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하위 분류기 최적화 및 학습 시작...\n",
      "\n",
      "--- [Mystery] 그룹 전용 분류기 최적화 중 ---\n",
      "  -> 최적 설정: alpha=0.00007, penalty=l2\n",
      "  -> 테스트 정확도: 0.9096\n",
      "--- [Dev] 그룹 전용 분류기 최적화 중 ---\n",
      "  -> 최적 설정: alpha=0.00031, penalty=elasticnet\n",
      "  -> 테스트 정확도: 0.8754\n",
      "--- [Culture] 그룹 전용 분류기 최적화 중 ---\n",
      "  -> 최적 설정: alpha=0.00001, penalty=l1\n",
      "  -> 테스트 정확도: 0.9866\n",
      "\n",
      " 모든 하위 분류기 학습 완료.\n"
     ]
    }
   ],
   "source": [
    "# 세부 레딧 분류기 학습\n",
    "# 그룹별로 별도의 분류기를 저장할 딕셔너리\n",
    "sub_classifiers = {}\n",
    "sub_best_params = {}\n",
    "\n",
    "# 최적화 탐색 범위 설정 (RandomizedSearch용)\n",
    "param_dist = {\n",
    "    'loss': ['log_loss'], \n",
    "    'alpha': loguniform(1e-5, 1e-1), # 0.00001 ~ 0.1 사이 랜덤 탐색\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'max_iter': [1000, 2000]\n",
    "}\n",
    "\n",
    "print(\"하위 분류기 최적화 및 학습 시작...\\n\")\n",
    "\n",
    "y_pred_sub = [] # 하위 분류기 예측값 저장 리스트\n",
    "for group_name, sub_list in GROUP_MAP.items():\n",
    "    print(f\"--- [{group_name}] 그룹 전용 분류기 최적화 중 ---\")\n",
    "    \n",
    "    # 해당 그룹 데이터 추출\n",
    "    group_mask = preprocessed_df['subreddit'].isin(sub_list)\n",
    "    \n",
    "    X_group = X[group_mask.values]\n",
    "    y_group_sub = preprocessed_df.loc[group_mask, 'subreddit']\n",
    "    \n",
    "    # 데이터 분할\n",
    "    X_g_train, X_g_test, y_g_train, y_g_test = train_test_split(\n",
    "        X_group, y_group_sub, test_size=0.2, random_state=42, stratify=y_group_sub\n",
    "    )\n",
    "    \n",
    "    # RandomizedSearchCV 설정\n",
    "    base_model = SGDClassifier(random_state=42, class_weight='balanced', n_jobs=1)\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=10,          # 그룹당 10번 탐색\n",
    "        cv=3,               # 그룹별 3-Fold 사용\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # 최적화 실행\n",
    "    search.fit(X_g_train, y_g_train)\n",
    "    \n",
    "    # 최적 모델 저장 및 평가\n",
    "    best_model = search.best_estimator_\n",
    "    sub_classifiers[group_name] = best_model\n",
    "    sub_best_params[group_name] = search.best_params_\n",
    "    \n",
    "    # 검증 데이터로 성능 확인\n",
    "    y_g_pred = best_model.predict(X_g_test)\n",
    "    y_pred_sub.append(y_g_pred[0])\n",
    "    acc = accuracy_score(y_g_test, y_g_pred)\n",
    "    \n",
    "    print(f\"  -> 최적 설정: alpha={search.best_params_['alpha']:.5f}, penalty={search.best_params_['penalty']}\")\n",
    "    print(f\"  -> 테스트 정확도: {acc:.4f}\")\n",
    "\n",
    "print(\"\\n 모든 하위 분류기 학습 완료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7891268c",
   "metadata": {},
   "source": [
    "## 4. 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "561b8799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "import joblib\n",
    "\n",
    "# 벡터라이저 저장\n",
    "joblib.dump(vectorizer, MODEL_PATH + 'tfidf_vectorizer.pkl')\n",
    "\n",
    "# 상위 분류기 모델 저장\n",
    "best_top_classifier = random_search_top.best_estimator_\n",
    "joblib.dump(best_top_classifier, MODEL_PATH + 'top_classifier.pkl')\n",
    "\n",
    "# 하위 분류기 모델 저장\n",
    "for group_name, model in sub_classifiers.items():\n",
    "    joblib.dump(model, MODEL_PATH + f'sub_classifier_{group_name}.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
